2025-08-13 22:55:44,475 - INFO - Loading tokenizer...
2025-08-13 22:55:45,929 - INFO - Loading base model in 4-bit...
2025-08-13 22:55:47,728 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-08-13 22:55:55,430 - INFO - Preparing model for training...
2025-08-13 22:55:55,533 - INFO - Loading and tokenizing dataset...
2025-08-13 22:56:08,190 - INFO - Applying LoRA configuration...
2025-08-13 22:56:08,353 - INFO - Initializing trainer...
2025-08-13 22:56:08,390 - INFO - Starting training...
2025-08-13 23:00:20,981 - INFO - Saving LoRA adapter and tokenizer...
2025-08-13 23:00:22,049 - INFO - Merging LoRA weights into base model...
2025-08-13 23:00:22,050 - INFO - Loading base model in 4-bit for merging...
2025-08-13 23:00:22,662 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-08-13 23:05:59,093 - INFO - Loading tokenizer...
2025-08-13 23:06:14,345 - INFO - Loading base model in 4-bit...
2025-08-13 23:06:15,744 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-08-13 23:06:24,340 - INFO - Preparing model for training...
2025-08-13 23:06:24,443 - INFO - Loading and tokenizing dataset...
2025-08-13 23:06:26,704 - INFO - Applying LoRA configuration...
2025-08-13 23:06:26,959 - INFO - Initializing trainer...
2025-08-13 23:06:26,981 - INFO - Starting training...
2025-08-13 23:11:09,003 - INFO - Saving LoRA adapter and tokenizer...
2025-08-13 23:11:09,830 - INFO - LoRA adapter and tokenizer saved at: E:\CSI_CB\fine_tuned_model
2025-08-13 23:11:09,830 - INFO - Merging LoRA weights into base model...
2025-08-13 23:11:09,830 - INFO - Loading base model on CPU...
2025-08-13 23:11:18,002 - INFO - Loading LoRA adapters...
2025-08-13 23:11:18,306 - INFO - Merging weights...
2025-08-13 23:11:19,151 - INFO - Saving merged model...
2025-08-13 23:11:43,620 - INFO - Final merged model saved at: E:\CSI_CB\final_merged_model
2025-08-13 23:11:43,675 - INFO - Providing inference guidance...
2025-08-18 17:17:22,576 - INFO - Loading tokenizer...
2025-08-18 17:17:24,943 - INFO - Loading base model in 4-bit...
2025-08-18 17:17:26,654 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-08-18 17:17:36,793 - INFO - Preparing model for training...
2025-08-18 17:17:36,893 - INFO - Loading and tokenizing dataset...
2025-08-18 17:17:40,217 - INFO - Applying LoRA configuration...
2025-08-18 17:17:40,450 - INFO - Initializing trainer...
2025-08-18 17:17:40,504 - INFO - Starting training...
2025-08-18 17:25:58,277 - INFO - Saving LoRA adapter and tokenizer...
2025-08-18 17:25:59,267 - INFO - LoRA adapter and tokenizer saved at: E:\CSI_CB\fine_tuned_model
2025-08-18 17:25:59,267 - INFO - Merging LoRA weights into base model...
2025-08-18 17:25:59,267 - INFO - Loading base model on CPU...
2025-08-18 17:26:08,312 - INFO - Loading LoRA adapters...
2025-08-18 17:26:08,623 - INFO - Merging weights...
2025-08-18 17:26:09,360 - INFO - Saving merged model...
2025-08-18 17:26:41,797 - INFO - Final merged model saved at: E:\CSI_CB\final_merged_model
2025-08-18 17:26:41,844 - INFO - Providing inference guidance...
2025-08-18 18:21:33,015 - INFO - Loading tokenizer...
2025-08-18 18:21:34,590 - INFO - Loading base model in 4-bit...
2025-08-18 18:21:35,630 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-08-18 18:21:43,609 - INFO - Preparing model for training...
2025-08-18 18:21:43,710 - INFO - Loading and tokenizing dataset...
2025-08-18 18:21:46,120 - INFO - Applying LoRA configuration...
2025-08-18 18:21:46,270 - INFO - Initializing trainer...
2025-08-18 18:21:46,292 - INFO - Starting training...
2025-08-18 19:09:27,304 - INFO - Loading tokenizer...
2025-08-18 19:09:38,282 - INFO - Loading base model in 4-bit...
2025-08-18 19:09:46,871 - INFO - Preparing model for training...
2025-08-18 19:09:46,969 - INFO - Loading and tokenizing dataset...
2025-08-18 19:10:09,042 - INFO - Applying LoRA configuration...
2025-08-18 19:10:09,196 - INFO - Initializing trainer...
2025-08-18 19:10:09,215 - INFO - Starting training...
2025-08-18 19:10:10,777 - ERROR - Training failed: You can't train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device()}` or `device_map={'':torch.xpu.current_device()}`
2025-08-18 19:12:53,317 - INFO - Loading tokenizer...
2025-08-18 19:12:54,324 - INFO - Loading base model in 4-bit...
2025-08-18 19:12:55,386 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-08-18 19:13:03,423 - INFO - Preparing model for training...
2025-08-18 19:13:03,524 - INFO - Loading and tokenizing dataset...
2025-08-18 19:13:06,153 - INFO - Applying LoRA configuration...
2025-08-18 19:13:06,297 - INFO - Initializing trainer...
2025-08-18 19:13:06,317 - INFO - Starting training...
2025-08-18 22:54:43,856 - INFO - Loading tokenizer...
2025-08-18 22:54:44,898 - INFO - Loading base model in 4-bit...
2025-08-18 22:54:46,155 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-08-18 22:54:54,419 - INFO - Preparing model for training...
2025-08-18 22:54:54,520 - INFO - Loading and tokenizing dataset...
2025-08-18 22:54:57,062 - INFO - Applying LoRA configuration...
2025-08-18 22:54:57,253 - INFO - Initializing trainer...
2025-08-18 22:54:57,377 - INFO - Starting training...
2025-08-19 00:12:10,612 - INFO - Saving LoRA adapter and tokenizer...
2025-08-19 00:12:11,570 - INFO - LoRA adapter and tokenizer saved at: E:\CSI_CB\fine_tuned_model
2025-08-19 00:12:11,570 - INFO - Merging LoRA weights into base model...
2025-08-19 00:12:11,570 - INFO - Loading base model on CPU...
2025-08-19 00:12:20,842 - INFO - Loading LoRA adapters...
2025-08-19 00:12:21,063 - INFO - Merging weights...
2025-08-19 00:12:21,848 - INFO - Saving merged model...
2025-08-19 00:12:46,797 - INFO - Final merged model saved at: E:\CSI_CB\final_merged_model
2025-08-19 00:12:46,925 - INFO - Providing inference guidance...
2025-08-19 15:33:45,591 - INFO - Loading tokenizer...
2025-08-19 15:33:46,425 - INFO - Loading base model in 4-bit...
2025-08-19 15:33:47,745 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-08-19 15:33:55,207 - INFO - Preparing model for training...
2025-08-19 15:33:55,311 - INFO - Loading and tokenizing dataset...
2025-08-19 15:34:07,559 - INFO - Applying LoRA configuration...
2025-08-19 15:34:07,723 - INFO - Initializing trainer...
2025-08-19 15:34:07,807 - INFO - Starting training...
2025-08-19 17:50:37,153 - INFO - Loading tokenizer...
2025-08-19 17:50:38,200 - INFO - Loading base model in 4-bit...
2025-08-19 17:50:39,329 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-08-19 17:50:47,225 - INFO - Preparing model for training...
2025-08-19 17:50:47,326 - INFO - Loading and tokenizing dataset...
2025-08-19 17:51:29,421 - INFO - Applying LoRA configuration...
2025-08-19 17:51:29,578 - INFO - Initializing trainer...
2025-08-19 17:51:29,604 - INFO - Starting training...
2025-08-19 19:08:42,873 - INFO - Saving LoRA adapter and tokenizer...
2025-08-19 19:08:44,073 - INFO - LoRA adapter and tokenizer saved at: E:\CSI_CB\fine_tuned_model
2025-08-19 19:08:44,073 - INFO - Merging LoRA weights into base model...
2025-08-19 19:08:44,074 - INFO - Loading base model on CPU...
2025-08-19 19:08:55,497 - INFO - Loading LoRA adapters...
2025-08-19 19:08:55,772 - INFO - Merging weights...
2025-08-19 19:08:56,539 - INFO - Saving merged model...
2025-08-19 19:09:24,990 - INFO - Final merged model saved at: E:\CSI_CB\final_merged_model
2025-08-19 19:09:25,025 - INFO - Providing inference guidance...
